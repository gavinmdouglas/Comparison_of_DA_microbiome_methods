---
title: "Figure1"
author: "Jacob T. Nearing"
date: "1/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Libraries
library(pheatmap)
library(gridExtra)
# Set WD
opts_knit$set(root.dir = '/home/jacob/projects/Hackathon/Studies/')
setwd("/home/jacob/projects/Hackathon/Studies/")



###updated dataset names
Data_set_names <- c(ArcticFireSoils="Soil - Fires",
                         ArcticFreshwaters="Freshwater - Arctic",
                         ArcticTransects="Soil - Arctic",
                         art_scher="Human - RA",
                         asd_son= "Human - ASD",
                         BISCUIT= "Human - CD (1)",
                         Blueberry= "Soil - Blueberry",
                         cdi_schubert="Human - C. diff (1)",
                         cdi_vincent="Human - C. diff (2)",
                         Chemerin="Mouse Facilities",
                         crc_baxter="Human - CC (1)",
                         crc_zeller="Human - CC (2)",
                         edd_singh="Human - Inf.",
                         Exercise="Mouse - Exercised",
                         glass_plastic_oberbeckmann="Marine - Plastic (4)",
                         GWMC_ASIA_NA="WWSR - Continents",
                         GWMC_HOT_COLD="WWSR - Temp.",
                         hiv_dinh="Human - HIV (1)",
                         hiv_lozupone="Human - HIV (2)",
                         hiv_noguerajulian="Human - HIV (3)",
                         ibd_gevers="Human - CD (2)",
                         ibd_papa="Human - IBD",
                         Ji_WTP_DS="Freshwater - Treat.",
                         MALL="Human - ALL",
                         ob_goodrich="Human - OB (1)",
                         ob_ross="Human - OB (2)",
                         ob_turnbaugh="Human - OB (3)",
                         ob_zhu="Human - OB (4)",
                         ###"ob_zupancic",
                         Office="Built - Office",
                         par_scheperjans="Human - Par.",
                         sed_plastic_hoellein="Marine - Plastic (2)",
                         sed_plastic_rosato="Marine - Plastic (5)",
                         seston_plastic_mccormick="River - Plastic",
                         sw_plastic_frere="Marine - Plastic (1)",
                         sw_sed_detender="Marine - Sediment",
                          t1d_alkanani="Human - T1D (1)",
                         t1d_mejialeon="Human - T1D (2)",
                         wood_plastic_kesy="Marine - Plastic (3)")
```

# Functions

```{r}
read_table_and_check_line_count <- function(filepath, ...) {
  # Function to read in table and to check whether the row count equals the expected line count of the file.
  
  exp_count <- as.numeric(sub(pattern = " .*$", "", system(command = paste("wc -l", filepath, sep=" "), intern = TRUE)))
  
  df <- read.table(filepath, ...)
  
  if(length(grep("^V", colnames(df))) != ncol(df)) {
    exp_count <- exp_count - 1
  }
  
  if(exp_count != nrow(df)) {
    stop(paste("Expected ", as.character(exp_count), " lines, but found ", as.character(nrow(df))))
  } else {
    return(df) 
  }
}


read_hackathon_results <- function(study,
                                   results_folder="Fix_Results_0.1") {
  
  da_tool_filepath <- list()
  da_tool_filepath[["aldex2"]] <- paste(study, results_folder, "Aldex_out/Aldex_res.tsv", sep = "/")
  da_tool_filepath[["ancom"]] <- paste(study, results_folder, "ANCOM_out/Ancom_res.tsv", sep = "/")
  da_tool_filepath[["corncob"]] <- paste(study, results_folder, "Corncob_out/Corncob_results.tsv", sep = "/")
  da_tool_filepath[["deseq2"]] <- paste(study, results_folder, "Deseq2_out/Deseq2_results.tsv", sep = "/")
  da_tool_filepath[["edger"]] <- paste(study, results_folder, "edgeR_out/edgeR_res.tsv", sep = "/")
  da_tool_filepath[["lefse"]] <- paste(study, results_folder, "Lefse_out/Lefse_results.tsv", sep = "/")
  da_tool_filepath[["maaslin2"]] <- paste(study, results_folder, "Maaslin2_out/all_results.tsv", sep = "/")
  da_tool_filepath[["maaslin2rare"]] <- paste(study, results_folder, "Maaslin2_rare_out/all_results.tsv", sep = "/")
  da_tool_filepath[["metagenomeSeq"]] <- paste(study, results_folder, "metagenomeSeq_out/mgSeq_res.tsv", sep = "/")
  da_tool_filepath[["ttestrare"]] <- paste(study, results_folder, "t_test_rare_out/t_test_res.tsv", sep = "/")
  da_tool_filepath[["wilcoxonclr"]] <- paste(study, results_folder, "Wilcoxon_CLR_out/Wil_CLR_results.tsv", sep = "/")
  da_tool_filepath[["wilcoxonrare"]] <- paste(study, results_folder, "Wilcoxon_rare_out/Wil_rare_results.tsv", sep = "/")
  da_tool_filepath[["limma_voom_TMM"]] <- paste(study, results_folder, "limma_voom_tmm_out/limma_voom_tmm_res.tsv", sep="/")
  da_tool_filepath[["limma_voom_TMMwsp"]] <- paste(study, results_folder, "Limma_voom_TMMwsp/limma_voom_tmmwsp_res.tsv", sep="/")
  
  adjP_colname <- list()
  adjP_colname[["aldex2"]] <- "wi.eBH"
  adjP_colname[["ancom"]] <- "detected_0.9"
  adjP_colname[["corncob"]] <- "x"
  adjP_colname[["deseq2"]] <- "padj"
  adjP_colname[["edger"]] <- "FDR"
  adjP_colname[["lefse"]] <- "V5"
  adjP_colname[["maaslin2"]] <- "qval"
  adjP_colname[["maaslin2rare"]] <- "qval"
  adjP_colname[["metagenomeSeq"]] <- "adjPvalues"
  adjP_colname[["ttestrare"]] <- "x"
  adjP_colname[["wilcoxonclr"]] <- "x"
  adjP_colname[["wilcoxonrare"]] <- "x"
  adjP_colname[["limma_voom_TMM"]] <- "adj.P.Val"
  adjP_colname[["limma_voom_TMMwsp"]] <- "adj.P.Val"
  # Read in results files and run sanity check that results files have expected number of lines
  da_tool_results <- list()
  
  missing_tools <- c()
  
  for(da_tool in names(da_tool_filepath)) {
    
    if(! (file.exists(da_tool_filepath[[da_tool]]))) {
       missing_tools <- c(missing_tools, da_tool)
       message(paste("File ", da_tool_filepath[[da_tool]], " not found. Skipping.", sep=""))
       next
    }
    
    if(da_tool %in% c("ancom", "maaslin2", "maaslin2rare")) {
      da_tool_results[[da_tool]] <- read_table_and_check_line_count(da_tool_filepath[[da_tool]], sep="\t", row.names=2, header=TRUE)
    } else if(da_tool == "lefse") {
      da_tool_results[[da_tool]] <- read_table_and_check_line_count(da_tool_filepath[[da_tool]], sep="\t", row.names=1, header=FALSE, stringsAsFactors=FALSE)
      rownames(da_tool_results[[da_tool]]) <- gsub("^f_", "", rownames(da_tool_results[[da_tool]]))
    } else {
      da_tool_results[[da_tool]] <- read_table_and_check_line_count(da_tool_filepath[[da_tool]], sep="\t", row.names=1, header=TRUE)
    }
  }
  
  # Combine corrected P-values into same table.
  all_rows <- c()
  
   for(da_tool in names(adjP_colname)) {
     all_rows <- c(all_rows, rownames(da_tool_results[[da_tool]]))
   }
  all_rows <- all_rows[-which(duplicated(all_rows))]

  adjP_table <- data.frame(matrix(NA, ncol=length(names(da_tool_results)), nrow=length(all_rows)))
  colnames(adjP_table) <- names(da_tool_results)
  rownames(adjP_table) <- all_rows
  
  for(da_tool in colnames(adjP_table)) {
 
    if(da_tool %in% missing_tools) {
       next
    }
    
    if(da_tool == "lefse") {
     
        tmp_lefse <- da_tool_results[[da_tool]][, adjP_colname[[da_tool]]]
        tmp_lefse[which(tmp_lefse == "-")] <- NA
        adjP_table[rownames(da_tool_results[[da_tool]]), da_tool] <- as.numeric(tmp_lefse)

        lefse_tested_asvs <- rownames(da_tool_results$wilcoxonrare)[which(! is.na(da_tool_results$wilcoxonrare))]
        lefse_NA_asvs <- rownames(da_tool_results$lefse)[which(is.na(tmp_lefse))]
  
        adjP_table[lefse_NA_asvs[which(lefse_NA_asvs %in% lefse_tested_asvs)], da_tool] <- 1
        
    } else if(da_tool == "ancom") {
      
      sig_ancom_hits <- which(da_tool_results[[da_tool]][, adjP_colname[[da_tool]]])
      ancom_results <- rep(1, length(da_tool_results[[da_tool]][, adjP_colname[[da_tool]]]))
      ancom_results[sig_ancom_hits] <- 0
      adjP_table[rownames(da_tool_results[[da_tool]]), da_tool] <- ancom_results
    
    } else if(da_tool %in% c("wilcoxonclr", "wilcoxonrare", "ttestrare")) {
      
      # Need to perform FDR-correction on these outputs.
      adjP_table[rownames(da_tool_results[[da_tool]]), da_tool] <- p.adjust(da_tool_results[[da_tool]][, adjP_colname[[da_tool]]], "fdr")
    
    } else {
      adjP_table[rownames(da_tool_results[[da_tool]]), da_tool] <- da_tool_results[[da_tool]][, adjP_colname[[da_tool]]]
    }
  }

  return(list(raw_tables=da_tool_results,
              adjP_table=adjP_table))
  
}

```

# Read in data

## Results
```{r}
hackathon_study_ids <- c("ArcticFireSoils",
                         "ArcticFreshwaters",
                         "ArcticTransects",
                         "art_scher",
                         "asd_son",
                         "BISCUIT",
                         "Blueberry",
                         "cdi_schubert",
                         "cdi_vincent",
                         "Chemerin",
                         "crc_baxter",
                         "crc_zeller",
                         "edd_singh",
                         "Exercise",
                         "glass_plastic_oberbeckmann",
                         "GWMC_ASIA_NA",
                         "GWMC_HOT_COLD",
                         "hiv_dinh",
                         "hiv_lozupone",
                         "hiv_noguerajulian",
                         "ibd_gevers",
                         "ibd_papa",
                         "Ji_WTP_DS",
                         "MALL",
                         "ob_goodrich",
                         "ob_ross",
                         "ob_turnbaugh",
                         "ob_zhu",
                         ###"ob_zupancic",
                         "Office",
                         "par_scheperjans",
                         "sed_plastic_hoellein",
                         "sed_plastic_rosato",
                         "seston_plastic_mccormick",
                         "sw_plastic_frere",
                         "sw_sed_detender",
                          "t1d_alkanani",
                         "t1d_mejialeon",
                         "wood_plastic_kesy")

filt_results <- lapply(hackathon_study_ids, read_hackathon_results)
names(filt_results) <- hackathon_study_ids


unfilt_results <- lapply(hackathon_study_ids, read_hackathon_results, results_folder = "No_filt_Results")
names(unfilt_results) <- hackathon_study_ids

saveRDS(filt_results, "~/GitHub_Repos/Hackathon_testing/Data/Filt_results.RDS")
saveRDS(unfilt_results, "~/GitHub_Repos/Hackathon_testing/Data/Unfilt_results.RDS")

```

## ASV tables
```{r}
filt_study_tab <- list()
filt_study_tab[["rare"]] <- list()
filt_study_tab[["nonrare"]] <- list()

Read_study_table <- function(address, grp_address){
  con <- file(address)
  file_1_line1 <- readLines(address,n=1)
  close(con)

  if(grepl("Constructed from biom file", file_1_line1)){
    ASV_table <- read.table(address, sep="\t", skip=1, header=T, row.names = 1,
                          comment.char = "", quote="", check.names = F)
  }else{
    ASV_table <- read.table(address, sep="\t", header=T, row.names = 1,
                          comment.char = "", quote="", check.names = F)
  }
  ## read in groupings data and filter table
  groupings <- read.table(grp_address, sep="\t", row.names = 1, header=T, comment.char = "", quote="", check.names = F)

  #number of samples
  sample_num <- length(colnames(ASV_table))
  grouping_num <- length(rownames(groupings))

  #check if sample number is the same
  # if they are not then get intersect and filter ASV table to only include those in grp file
  if(sample_num != grouping_num){
      rows_to_keep <- intersect(colnames(ASV_table), rownames(groupings))
      ASV_table <- ASV_table[,rows_to_keep]
  }
  return(ASV_table)
}

for (study in hackathon_study_ids){
  #get file for grping file
  grp_file_name <- list.files(path=paste(study,"/", sep=""), pattern = "_meta.*(.tsv)|(.csv)")
  if(rlang::is_empty(grp_file_name)){
    message("Grouping file was not located for ",study)
  }else{
      filt_study_tab[["nonrare"]][[study]] <- Read_study_table(paste(study, "/Fix_Results_0.1/fixed_non_rare_tables/", study, "_ASVs_table.tsv", sep=""),
                                                           paste(study, "/",grp_file_name, sep=""))
      filt_study_tab[["rare"]][[study]] <- Read_study_table(paste(study, "/Fix_Results_0.1/fixed_rare_tables/", study, "_ASVs_table.tsv", sep=""),
                                                            paste(study,"/",grp_file_name, sep=""))
  }
}

unfilt_study_tab <- list()
unfilt_study_tab[["rare"]] <- list()
unfilt_study_tab[["nonrare"]] <- list()

for (study in hackathon_study_ids){
  
  
   #get file for grping file
  grp_file_name <- list.files(path=paste(study,"/", sep=""), pattern = "_meta.*(.tsv)|(.csv)")
  if(rlang::is_empty(grp_file_name)){
    message("Grouping file was not located for ",study)
  }else{
      unfilt_study_tab[["nonrare"]][[study]] <- Read_study_table(paste(study, "/No_filt_Results/fixed_non_rare_tables/", study, "_ASVs_table.tsv", sep=""),
                                                           paste(study, "/",grp_file_name, sep=""))
      unfilt_study_tab[["rare"]][[study]] <- Read_study_table(paste(study, "/", study, "_ASVs_table_rare.tsv", sep=""),
                                                            paste(study,"/",grp_file_name, sep=""))
  }
}

saveRDS(unfilt_study_tab, "~/GitHub_Repos/Hackathon_testing/Data/unfilt_study_tab.RDS")
saveRDS(filt_study_tab, "~/GitHub_Repos/Hackathon_testing/Data/filt_study_tab.RDS")
```

# Figure 1 Analysis

## Read in data
```{r}
filt_results <- readRDS("~/GitHub_Repos/Hackathon_testing/Data/Filt_results.RDS")
unfilt_results <- readRDS("~/GitHub_Repos/Hackathon_testing/Data/Unfilt_results.RDS")

unfilt_study_tab  <- readRDS("~/GitHub_Repos/Hackathon_testing/Data/unfilt_study_tab.RDS")
filt_study_tab <- readRDS("~/GitHub_Repos/Hackathon_testing/Data/filt_study_tab.RDS")

```


## Calculate dataset stats

### Function
```{r, function}
Calc_Other_community_metrics <- function(List_tab){
  Dataset_Char <- list()
  Dataset_Char[["rare"]] <- list()
  Dataset_Char[["nonrare"]] <- list()
  ## Calculate Sparsity
  
  message("Calculating Sparsity")
  ## deal with rare tables first
  for(study in names(List_tab[["rare"]])){
    #get current study table
    study_table <- List_tab[["rare"]][[study]]
    #get total number of cells
    total_cells <- dim(study_table)[1] * dim(study_table)[2]
    #get sparsity
    Sparsity <- length(which(study_table==0))/total_cells
    #assign Sparsity to data.
    Dataset_Char[["rare"]][[study]][["Sparsity"]] <- Sparsity
  }
  ## deal with nonrare tables
    for(study in names(List_tab[["nonrare"]])){
    #get current study table
    study_table <- List_tab[["nonrare"]][[study]]
    #get total number of cells
    total_cells <- dim(study_table)[1] * dim(study_table)[2]
    #get sparsity
    Sparsity <- length(which(study_table==0))/total_cells
    #assign Sparsity to data.
    Dataset_Char[["nonrare"]][[study]][["Sparsity"]] <- Sparsity
  }
  
  message("Calculating Number of Features")
  ## Calculate number of features
  # deal with rare tables
  for(study in names(List_tab[["rare"]])){
    study_table <- List_tab[["rare"]][[study]]
    #tables are features x samples
    num_feats <- dim(study_table)[1]
    
    Dataset_Char[["rare"]][[study]][["Number of Features"]] <- num_feats
  }
  # deal with nonrare tables
  for(study in names(List_tab[["nonrare"]])){
    study_table <- List_tab[["nonrare"]][[study]]
    num_feats <- dim(study_table)[1]
    Dataset_Char[["nonrare"]][[study]][["Number of Features"]] <- num_feats
  }

  message("Calculating median read depth")
  ## Calculate median read depth
  #deal with rare tables
  for(study in names(List_tab[["rare"]])){
    study_table <- List_tab[["rare"]][[study]]
    median_depth <- median(colSums(study_table))
    Dataset_Char[["rare"]][[study]][["Median Depth"]] <- median_depth
  }
  #deal with nonrare tables
  for(study in names(List_tab[["nonrare"]])){
    study_table <- List_tab[["nonrare"]][[study]]
    median_depth <- median(colSums(study_table))
    Dataset_Char[["nonrare"]][[study]][["Median Depth"]] <- median_depth
  }
  message("Calculating median richness")
  ## Calculate median richness
  #deal with rare tables
  for(study in names(List_tab[["rare"]])){
    study_table <- List_tab[["rare"]][[study]]
    binary_study_table <- study_table
    binary_study_table[binary_study_table > 0] <- 1
    median_richness <- median(colSums(binary_study_table))
    Dataset_Char[["rare"]][[study]][["Median Richness"]] <- median_richness
  }
  #deal with nonrare tables
  for(study in names(List_tab[["nonrare"]])){
    study_table <- List_tab[["nonrare"]][[study]]
    binary_study_table <- study_table
    binary_study_table[binary_study_table > 0] <- 1
    median_richness <- median(colSums(binary_study_table))
    Dataset_Char[["nonrare"]][[study]][["Median Richness"]] <- median_richness
  }
  
  message("Calculating depth range")
  for(study in names(List_tab[["nonrare"]])){
    study_table <- List_tab[["nonrare"]][[study]]
    max_depth <- max(colSums(study_table))
    min_depth <- min(colSums(study_table))
    depth_range <- max_depth - min_depth
    Dataset_Char[["nonrare"]][[study]][["Depth Range"]] <- depth_range
    
  }
  
  message("Calculate CoV for depth range")
  for(study in names(List_tab[["nonrare"]])){
    study_table <- List_tab[["nonrare"]][[study]]
    mean_depth <- mean(colSums(study_table))
    std_depth <- sd(colSums(study_table))
    Cov <- std_depth/mean_depth
    Dataset_Char[["nonrare"]][[study]][["Depth CoV"]] <- Cov
  }
  
  return(Dataset_Char)
}


```

### Calculate
```{r cal_dataset_char}
filt_community_metrics <- Calc_Other_community_metrics(filt_study_tab)
unfilt_community_metrics <- Calc_Other_community_metrics(unfilt_study_tab)

### generate tables
filt_rare_metrics_df <- do.call(rbind, filt_community_metrics[[1]])
filt_nonrare_metrics_df <- do.call(rbind, filt_community_metrics[[2]])

unfilt_rare_metrics_df <- do.call(rbind, unfilt_community_metrics[[1]])
unfilt_nonrare_metrics_df <- do.call(rbind, unfilt_community_metrics[[2]])

#scale 
scale_filt_rare_metrics_df <- scale(filt_rare_metrics_df)
scale_filt_nonrare_metrics_df <- scale(filt_nonrare_metrics_df)
scale_unfilt_rare_metrics_df <- scale(unfilt_rare_metrics_df)
scale_unfilt_nonrare_metrics_df <- scale(unfilt_nonrare_metrics_df)
```

### Get Number of Significant ASVs
```{r num_sig_ASV}
sig_counts <- data.frame(matrix(NA,
                                nrow=length(names(filt_results)),
                                ncol=ncol(filt_results[[1]]$adjP_table) + 1))
rownames(sig_counts) <- names(filt_results)
colnames(sig_counts) <- c("dataset", colnames(filt_results[[1]]$adjP_table))
sig_counts$dataset <- rownames(sig_counts)

filt_sig_counts <- sig_counts
filt_sig_percent <- sig_counts

unfilt_sig_counts <- sig_counts
unfilt_sig_percent <- sig_counts

for(study in rownames(filt_sig_counts)) {
  for(tool_name in colnames(filt_sig_counts)) {
    
    if(tool_name == "dataset") { next }

    if(! tool_name %in% colnames(filt_results[[study]]$adjP_table)) {
      filt_sig_counts[study, tool_name] <- NA
      filt_sig_percent[study, tool_name] <- NA
      next
    }
    
    filt_sig_counts[study, tool_name] <- length(which(filt_results[[study]]$adjP_table[, tool_name] < 0.05))
    
    # For rarified pipelines get total # ASVs from wilcoxonrare and for non-rarified get it from wilcoxonclr table.
    if(tool_name %in% c("lefse", "maaslin2rare", "ttestrare", "wilcoxonrare")) {
      filt_sig_percent[study, tool_name] <- (length(which(filt_results[[study]]$adjP_table[, tool_name] < 0.05)) / dim(filt_study_tab[["rare"]][[study]])[1]) * 100
    } else {
      filt_sig_percent[study, tool_name] <- (length(which(filt_results[[study]]$adjP_table[, tool_name] < 0.05)) / dim(filt_study_tab[["nonrare"]][[study]])[1]) * 100
    }
  }
}


for(study in rownames(unfilt_sig_counts)) {
  for(tool_name in colnames(unfilt_sig_counts)) {
    
    if(tool_name == "dataset") { next }

    if(! tool_name %in% colnames(unfilt_results[[study]]$adjP_table)) {
      unfilt_sig_counts[study, tool_name] <- NA
      unfilt_sig_percent[study, tool_name] <- NA
      next
    }
    
    unfilt_sig_counts[study, tool_name] <- length(which(unfilt_results[[study]]$adjP_table[, tool_name] < 0.05))
    
    # For rarified pipelines get total # ASVs from wilcoxonrare and for non-rarified get it from wilcoxonclr table.
    if(tool_name %in% c("lefse", "maaslin2rare", "ttestrare", "wilcoxonrare")) {
      unfilt_sig_percent[study, tool_name] <- (length(which(unfilt_results[[study]]$adjP_table[, tool_name] < 0.05)) / dim(unfilt_study_tab[["rare"]][[study]])[1]) * 100
    } else {
      unfilt_sig_percent[study, tool_name] <- (length(which(unfilt_results[[study]]$adjP_table[, tool_name] < 0.05)) / dim(unfilt_study_tab[["nonrare"]][[study]])[1]) * 100
    }
  }
}

```

### Mean percent across all datasets

Across all filtered datasets:

```{r print_mean_per_filt}
filt_mean_percent <- print(sort(colSums(filt_sig_percent[, -1], na.rm = TRUE) / (colSums(! is.na(filt_sig_percent[, -1])))))
```

Across all unfiltered datasets:

```{r print_mean_per_unfilt}
unfilt_mean_percent <- print(sort(colSums(unfilt_sig_percent[, -1], na.rm = TRUE) / (colSums(! is.na(unfilt_sig_percent[, -1])))))
```


### HeatMaps

#### Filter
```{r}

filt_sig_percent_scaled <- data.frame(scale(t(filt_sig_percent[, -1]), center = TRUE, scale = TRUE))

hackathon_metadata <- read.table("/home/jacob/GitHub_Repos/Hackathon_testing/Analysis_Scripts/Metadata/2020_06_18_Datasets_Hackathon.txt", header=TRUE, sep="\t", stringsAsFactors = FALSE, quote="")
rownames(hackathon_metadata) <- hackathon_metadata$Dataset.Name

## getting sample sizes from metadata sheet.... I'm not sure if these match exactly with the tables... we should double check this...



hackathon_metadata$log_N <- log(hackathon_metadata$Sample.Size)


aitchison_adonis <- readRDS(file = "/home/gavin/projects/hackathon/aitchison_adonis_no_ob_zupancic.rds")

hackathon_metadata$R.squared <- NA




for(dataset in names(aitchison_adonis)) {
  hackathon_metadata[dataset, "R.squared"] <- aitchison_adonis[[dataset]]$R2[1]
}

hackathon_metadata$log_R.squared <- log(hackathon_metadata$R.squared)

#remove ob_zupanic
fixed_hackathon_metadata <- hackathon_metadata[-23,]

## attach other dataset statistics

sort_filt_nonrare_metrics_df <- filt_nonrare_metrics_df[rownames(fixed_hackathon_metadata),,drop=F]
fixed_hackathon_metadata$Sparsity <- sort_filt_nonrare_metrics_df[,1]
fixed_hackathon_metadata$Richness <- sort_filt_nonrare_metrics_df[,4]
fixed_hackathon_metadata$log_Depth <- log(sort_filt_nonrare_metrics_df[,3])
fixed_hackathon_metadata$log_Depth_range <- log(sort_filt_nonrare_metrics_df[,5])
fixed_hackathon_metadata$CoV_Depth <- sort_filt_nonrare_metrics_df[,6]

colnames(filt_sig_percent_scaled)
test <- Data_set_names[colnames(filt_sig_percent_scaled)]


colnames(filt_sig_percent_scaled) <- test

rownames(fixed_hackathon_metadata) 
test2 <- Data_set_names[rownames(fixed_hackathon_metadata)]
test2

rownames(fixed_hackathon_metadata) <- test2

Alpha_order_filt <- filt_sig_percent_scaled[,order(colnames(filt_sig_percent_scaled))]

filt_ASV_nums <- pheatmap(Alpha_order_filt,
         clustering_method = "complete",
         legend=TRUE,
         display_numbers=t(filt_sig_counts[, -1]),
         annotation_col=fixed_hackathon_metadata[, c("log_N", "log_R.squared", "Sparsity", "Richness", "log_Depth",
                                                     "log_Depth_range", "CoV_Depth"), drop=FALSE],
         fontsize_row=14,
         legend_labels = "Percent of Significant Features",
         treeheight_col = 0,
         cluster_cols = F,
         cluster_rows = F,
         main="Filtered")
```

#### UnFilter
```{r}

unfilt_sig_percent_scaled <- data.frame(scale(t(unfilt_sig_percent[, -1]), center = TRUE, scale = TRUE))

hackathon_metadata <- read.table("/home/jacob/GitHub_Repos/Hackathon_testing/Analysis_Scripts/Metadata/2020_06_18_Datasets_Hackathon.txt", header=TRUE, sep="\t", stringsAsFactors = FALSE, quote="")
rownames(hackathon_metadata) <- hackathon_metadata$Dataset.Name

hackathon_metadata$log_N <- log(hackathon_metadata$Sample.Size)

aitchison_adonis <- readRDS(file = "/home/gavin/projects/hackathon/aitchison_adonis_no_ob_zupancic.rds")

hackathon_metadata$R.squared <- NA

for(dataset in names(aitchison_adonis)) {
  hackathon_metadata[dataset, "R.squared"] <- aitchison_adonis[[dataset]]$R2[1]
}

hackathon_metadata$log_R.squared <- log(hackathon_metadata$R.squared)

#remove ob_zupanic
fixed_hackathon_metadata <- hackathon_metadata[-23,]

## attach other dataset statistics

sort_unfilt_nonrare_metrics_df <- unfilt_nonrare_metrics_df[rownames(fixed_hackathon_metadata),,drop=F]
fixed_hackathon_metadata$Sparsity <- sort_unfilt_nonrare_metrics_df[,1]
fixed_hackathon_metadata$Richness <- sort_unfilt_nonrare_metrics_df[,4]
fixed_hackathon_metadata$log_Depth <- log(sort_unfilt_nonrare_metrics_df[,3])
fixed_hackathon_metadata$Log_Depth_range <- log(sort_unfilt_nonrare_metrics_df[,5])
fixed_hackathon_metadata$CoV_Depth <- sort_unfilt_nonrare_metrics_df[,6]


test <- Data_set_names[colnames(unfilt_sig_percent_scaled)]
colnames(unfilt_sig_percent_scaled) <- test

Alpha_order_unfilt <- unfilt_sig_percent_scaled[,order(colnames(unfilt_sig_percent_scaled))]



rownames(fixed_hackathon_metadata) 
test2 <- Data_set_names[rownames(fixed_hackathon_metadata)]
test2

rownames(fixed_hackathon_metadata) <- test2

unfilt_ASV_nums <- pheatmap(Alpha_order_unfilt,
         clustering_method = "complete",
         legend=TRUE,
         display_numbers=t(unfilt_sig_counts[, -1]),
         annotation_col=fixed_hackathon_metadata[, c("log_N", "log_R.squared", "Sparsity", "Richness", "log_Depth",
                                                     "Log_Depth_range", "CoV_Depth"), drop=FALSE],
         fontsize_row = 14,
         treeheight_col = 0,
         cluster_cols = F,
         cluster_rows = F,
         main="Unfiltered")


```



## Final Plot
```{r}
final_plot <- gridExtra::grid.arrange(filt_ASV_nums[[4]], unfilt_ASV_nums[[4]], nrow=2)
final_plot
```