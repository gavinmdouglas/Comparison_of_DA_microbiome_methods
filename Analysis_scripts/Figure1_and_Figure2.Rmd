---
title: "Figure1"
author: "Jacob T. Nearing"
date: "1/5/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Libraries
library(pheatmap)
library(gridExtra)
# Set WD


### SET DIRECTORY WITH DATA HERE
knitr::opts_knit$set(root.dir = '/home/jacob/projects/TEST_DA_TEST2/Hackathon/Studies/')
setwd("/home/jacob/projects/TEST_DA_TEST2/Hackathon/Studies/")

### Folders where final figures are written
### Make sure this is pointed to your home directory
display_items_out <- "/home/gavin/github_repos/hackathon/Comparison_of_DA_microbiome_methods/Display_items/"


###updated dataset names
Data_set_names <- c(ArcticFireSoils="Soil - Fires",
                         ArcticFreshwaters="Freshwater - Arctic",
                         ArcticTransects="Soil - Arctic",
                         art_scher="Human - RA",
                         asd_son= "Human - ASD",
                         BISCUIT= "Human - CD (1)",
                         Blueberry= "Soil - Blueberry",
                         cdi_schubert="Human - C. diff (1)",
                         cdi_vincent="Human - C. diff (2)",
                         Chemerin="Mouse Facilities",
                         crc_baxter="Human - CC (1)",
                         crc_zeller="Human - CC (2)",
                         edd_singh="Human - Inf.",
                         Exercise="Mouse - Exercised",
                         glass_plastic_oberbeckmann="Marine - Plastic (4)",
                         GWMC_ASIA_NA="WWSR - Continents",
                         GWMC_HOT_COLD="WWSR - Temp.",
                         hiv_dinh="Human - HIV (1)",
                         hiv_lozupone="Human - HIV (2)",
                         hiv_noguerajulian="Human - HIV (3)",
                         ibd_gevers="Human - CD (2)",
                         ibd_papa="Human - IBD",
                         Ji_WTP_DS="Freshwater - Treat.",
                         MALL="Human - ALL",
                         ob_goodrich="Human - OB (1)",
                         ob_ross="Human - OB (2)",
                         ob_turnbaugh="Human - OB (3)",
                         ob_zhu="Human - OB (4)",
                         ###"ob_zupancic",
                         Office="Built - Office",
                         par_scheperjans="Human - Par.",
                         sed_plastic_hoellein="Marine - Plastic (2)",
                         sed_plastic_rosato="Marine - Plastic (5)",
                         seston_plastic_mccormick="River - Plastic",
                         sw_plastic_frere="Marine - Plastic (1)",
                         sw_sed_detender="Marine - Sediment",
                          t1d_alkanani="Human - T1D (1)",
                         t1d_mejialeon="Human - T1D (2)",
                         wood_plastic_kesy="Marine - Plastic (3)")


tool_names <- c(aldex2="ALDEx2", ancom="ANCOM-II", corncob="corncob", deseq2="DESeq2", edger="edgeR", lefse="LEfSe", 
                limma_voom_TMM="limma voom (TMM)", limma_voom_TMMwsp="limma voom (TMMwsp)", maaslin2="MaAsLin2",
                maaslin2rare="MaAsLin2 (rare)", metagenomeSeq="metagenomeSeq", ttestrare="t-test (rare)", 
                wilcoxonclr="Wilcoxon (CLR)", wilcoxonrare="Wilcoxon (rare)")

Metadata_renames <- c(log_N="Log Sample Size", R.squared="Aitchson's Dist. Effect Size", log_R.squared="Log Aitchson's Dist. Effect Size", Sparsity="Sparsity", Richness="Richness", log_Depth="Log Max Read Depth", log_Depth_range="Log Read Depth Range", CoV_Depth="Read Depth Variation")
```

# Functions

```{r}
read_table_and_check_line_count <- function(filepath, ...) {
  # Function to read in table and to check whether the row count equals the expected line count of the file.
  
  exp_count <- as.numeric(sub(pattern = " .*$", "", system(command = paste("wc -l", filepath, sep=" "), intern = TRUE)))
  
  df <- read.table(filepath, ...)
  
  if(length(grep("^V", colnames(df))) != ncol(df)) {
    exp_count <- exp_count - 1
  }
  
  if(exp_count != nrow(df)) {
    stop(paste("Expected ", as.character(exp_count), " lines, but found ", as.character(nrow(df))))
  } else {
    return(df) 
  }
}


read_hackathon_results <- function(study,
                                   results_folder="Fix_Results_0.1") {
  
  da_tool_filepath <- list()
  da_tool_filepath[["aldex2"]] <- paste(study, results_folder, "Aldex_out/Aldex_res.tsv", sep = "/")
  da_tool_filepath[["ancom"]] <- paste(study, results_folder, "ANCOM_out/Ancom_res.tsv", sep = "/")
  da_tool_filepath[["corncob"]] <- paste(study, results_folder, "Corncob_out/Corncob_results.tsv", sep = "/")
  da_tool_filepath[["deseq2"]] <- paste(study, results_folder, "Deseq2_out/Deseq2_results.tsv", sep = "/")
  da_tool_filepath[["edger"]] <- paste(study, results_folder, "edgeR_out/edgeR_res.tsv", sep = "/")
  da_tool_filepath[["lefse"]] <- paste(study, results_folder, "Lefse_out/Lefse_results.tsv", sep = "/")
  da_tool_filepath[["maaslin2"]] <- paste(study, results_folder, "Maaslin2_out/all_results.tsv", sep = "/")
  da_tool_filepath[["maaslin2rare"]] <- paste(study, results_folder, "Maaslin2_rare_out/all_results.tsv", sep = "/")
  da_tool_filepath[["metagenomeSeq"]] <- paste(study, results_folder, "metagenomeSeq_out/mgSeq_res.tsv", sep = "/")
  da_tool_filepath[["ttestrare"]] <- paste(study, results_folder, "t_test_rare_out/t_test_res.tsv", sep = "/")
  da_tool_filepath[["wilcoxonclr"]] <- paste(study, results_folder, "Wilcoxon_CLR_out/Wil_CLR_results.tsv", sep = "/")
  da_tool_filepath[["wilcoxonrare"]] <- paste(study, results_folder, "Wilcoxon_rare_out/Wil_rare_results.tsv", sep = "/")
  da_tool_filepath[["limma_voom_TMM"]] <- paste(study, results_folder, "limma_voom_tmm_out/limma_voom_tmm_res.tsv", sep="/")
  da_tool_filepath[["limma_voom_TMMwsp"]] <- paste(study, results_folder, "Limma_voom_TMMwsp/limma_voom_tmmwsp_res.tsv", sep="/")
  
  adjP_colname <- list()
  adjP_colname[["aldex2"]] <- "wi.eBH"
  adjP_colname[["ancom"]] <- "detected_0.9"
  adjP_colname[["corncob"]] <- "x"
  adjP_colname[["deseq2"]] <- "padj"
  adjP_colname[["edger"]] <- "FDR"
  adjP_colname[["lefse"]] <- "V5"
  adjP_colname[["maaslin2"]] <- "qval"
  adjP_colname[["maaslin2rare"]] <- "qval"
  adjP_colname[["metagenomeSeq"]] <- "adjPvalues"
  adjP_colname[["ttestrare"]] <- "x"
  adjP_colname[["wilcoxonclr"]] <- "x"
  adjP_colname[["wilcoxonrare"]] <- "x"
  adjP_colname[["limma_voom_TMM"]] <- "adj.P.Val"
  adjP_colname[["limma_voom_TMMwsp"]] <- "adj.P.Val"
  # Read in results files and run sanity check that results files have expected number of lines
  da_tool_results <- list()
  
  missing_tools <- c()
  
  for(da_tool in names(da_tool_filepath)) {
    
    if(! (file.exists(da_tool_filepath[[da_tool]]))) {
       missing_tools <- c(missing_tools, da_tool)
       message(paste("File ", da_tool_filepath[[da_tool]], " not found. Skipping.", sep=""))
       next
    }
    
    if(da_tool %in% c("ancom", "maaslin2", "maaslin2rare")) {
      da_tool_results[[da_tool]] <- read_table_and_check_line_count(da_tool_filepath[[da_tool]], sep="\t", row.names=2, header=TRUE)
    } else if(da_tool == "lefse") {
      da_tool_results[[da_tool]] <- read_table_and_check_line_count(da_tool_filepath[[da_tool]], sep="\t", row.names=1, header=FALSE, stringsAsFactors=FALSE)
      rownames(da_tool_results[[da_tool]]) <- gsub("^f_", "", rownames(da_tool_results[[da_tool]]))
    } else {
      da_tool_results[[da_tool]] <- read_table_and_check_line_count(da_tool_filepath[[da_tool]], sep="\t", row.names=1, header=TRUE)
    }
  }
  
  # Combine corrected P-values into same table.
  all_rows <- c()
  
   for(da_tool in names(adjP_colname)) {
     all_rows <- c(all_rows, rownames(da_tool_results[[da_tool]]))
   }
  all_rows <- all_rows[-which(duplicated(all_rows))]

  adjP_table <- data.frame(matrix(NA, ncol=length(names(da_tool_results)), nrow=length(all_rows)))
  colnames(adjP_table) <- names(da_tool_results)
  rownames(adjP_table) <- all_rows
  
  for(da_tool in colnames(adjP_table)) {
 
    if(da_tool %in% missing_tools) {
       next
    }
    
    if(da_tool == "lefse") {
     
        tmp_lefse <- da_tool_results[[da_tool]][, adjP_colname[[da_tool]]]
        tmp_lefse[which(tmp_lefse == "-")] <- NA
        adjP_table[rownames(da_tool_results[[da_tool]]), da_tool] <- as.numeric(tmp_lefse)

        lefse_tested_asvs <- rownames(da_tool_results$wilcoxonrare)[which(! is.na(da_tool_results$wilcoxonrare))]
        lefse_NA_asvs <- rownames(da_tool_results$lefse)[which(is.na(tmp_lefse))]
  
        adjP_table[lefse_NA_asvs[which(lefse_NA_asvs %in% lefse_tested_asvs)], da_tool] <- 1
        
    } else if(da_tool == "ancom") {
      
      sig_ancom_hits <- which(da_tool_results[[da_tool]][, adjP_colname[[da_tool]]])
      ancom_results <- rep(1, length(da_tool_results[[da_tool]][, adjP_colname[[da_tool]]]))
      ancom_results[sig_ancom_hits] <- 0
      adjP_table[rownames(da_tool_results[[da_tool]]), da_tool] <- ancom_results
    
    } else if(da_tool %in% c("wilcoxonclr", "wilcoxonrare", "ttestrare")) {
      
      # Need to perform FDR-correction on these outputs.
      adjP_table[rownames(da_tool_results[[da_tool]]), da_tool] <- p.adjust(da_tool_results[[da_tool]][, adjP_colname[[da_tool]]], "fdr")
    
    } else {
      adjP_table[rownames(da_tool_results[[da_tool]]), da_tool] <- da_tool_results[[da_tool]][, adjP_colname[[da_tool]]]
    }
  }

  return(list(raw_tables=da_tool_results,
              adjP_table=adjP_table))
  
}

```

# Read in data

## Results
```{r}
hackathon_study_ids <- c("ArcticFireSoils",
                         "ArcticFreshwaters",
                         "ArcticTransects",
                         "art_scher",
                         "asd_son",
                         "BISCUIT",
                         "Blueberry",
                         "cdi_schubert",
                         "cdi_vincent",
                         "Chemerin",
                         "crc_baxter",
                         "crc_zeller",
                         "edd_singh",
                         "Exercise",
                         "glass_plastic_oberbeckmann",
                         "GWMC_ASIA_NA",
                         "GWMC_HOT_COLD",
                         "hiv_dinh",
                         "hiv_lozupone",
                         "hiv_noguerajulian",
                         "ibd_gevers",
                         "ibd_papa",
                         "Ji_WTP_DS",
                         "MALL",
                         "ob_goodrich",
                         "ob_ross",
                         "ob_turnbaugh",
                         "ob_zhu",
                         ###"ob_zupancic",
                         "Office",
                         "par_scheperjans",
                         "sed_plastic_hoellein",
                         "sed_plastic_rosato",
                         "seston_plastic_mccormick",
                         "sw_plastic_frere",
                         "sw_sed_detender",
                          "t1d_alkanani",
                         "t1d_mejialeon",
                         "wood_plastic_kesy")

filt_results <- lapply(hackathon_study_ids, read_hackathon_results)
names(filt_results) <- hackathon_study_ids


unfilt_results <- lapply(hackathon_study_ids, read_hackathon_results, results_folder = "No_filt_Results")
names(unfilt_results) <- hackathon_study_ids

saveRDS(filt_results, "/home/jacob/GitHub_Repos/Hackathon_testing/Data/Filt_results_TEST_DA_TEST2.RDS")
saveRDS(unfilt_results, "/home/jacob/GitHub_Repos/Hackathon_testing/Data/Unfilt_results_TEST_DA_TEST2.RDS")

```

## ASV tables
```{r}
filt_study_tab <- list()
filt_study_tab[["rare"]] <- list()
filt_study_tab[["nonrare"]] <- list()

Read_study_table <- function(address, grp_address){
  con <- file(address)
  file_1_line1 <- readLines(address,n=1)
  close(con)

  if(grepl("Constructed from biom file", file_1_line1)){
    ASV_table <- read.table(address, sep="\t", skip=1, header=T, row.names = 1,
                          comment.char = "", quote="", check.names = F)
  }else{
    ASV_table <- read.table(address, sep="\t", header=T, row.names = 1,
                          comment.char = "", quote="", check.names = F)
  }
  ## read in groupings data and filter table
  groupings <- read.table(grp_address, sep="\t", row.names = 1, header=T, comment.char = "", quote="", check.names = F)

  #number of samples
  sample_num <- length(colnames(ASV_table))
  grouping_num <- length(rownames(groupings))

  #check if sample number is the same
  # if they are not then get intersect and filter ASV table to only include those in grp file
  if(sample_num != grouping_num){
      rows_to_keep <- intersect(colnames(ASV_table), rownames(groupings))
      ASV_table <- ASV_table[,rows_to_keep]
  }
  return(ASV_table)
}

for (study in hackathon_study_ids){
  #get file for grping file
  grp_file_name <- list.files(path=paste(study,"/", sep=""), pattern = "_meta.*(.tsv)|(.csv)")
  if(rlang::is_empty(grp_file_name)){
    message("Grouping file was not located for ",study)
  }else{
      filt_study_tab[["nonrare"]][[study]] <- Read_study_table(paste(study, "/Fix_Results_0.1/fixed_non_rare_tables/", study, "_ASVs_table.tsv", sep=""),
                                                           paste(study, "/",grp_file_name, sep=""))
      filt_study_tab[["rare"]][[study]] <- Read_study_table(paste(study, "/Fix_Results_0.1/fixed_rare_tables/", study, "_ASVs_table.tsv", sep=""),
                                                            paste(study,"/",grp_file_name, sep=""))
  }
}

unfilt_study_tab <- list()
unfilt_study_tab[["rare"]] <- list()
unfilt_study_tab[["nonrare"]] <- list()

for (study in hackathon_study_ids){
  
  
   #get file for grping file
  grp_file_name <- list.files(path=paste(study,"/", sep=""), pattern = "_meta.*(.tsv)|(.csv)")
  if(rlang::is_empty(grp_file_name)){
    message("Grouping file was not located for ",study)
  }else{
      unfilt_study_tab[["nonrare"]][[study]] <- Read_study_table(paste(study, "/No_filt_Results/fixed_non_rare_tables/", study, "_ASVs_table.tsv", sep=""),
                                                           paste(study, "/",grp_file_name, sep=""))
      unfilt_study_tab[["rare"]][[study]] <- Read_study_table(paste(study, "/", study, "_ASVs_table_rare.tsv", sep=""),
                                                            paste(study,"/",grp_file_name, sep=""))
  }
}

saveRDS(unfilt_study_tab, "/home/jacob/GitHub_Repos/Hackathon_testing/Data/unfilt_study_tab_TEST_DA_TEST2.RDS")
saveRDS(filt_study_tab, "/home/jacob/GitHub_Repos/Hackathon_testing/Data/filt_study_tab_TEST_DA_TEST2.RDS")
```

# Figure 1 Analysis

## Read in data
```{r}
filt_results <- readRDS("/home/jacob/GitHub_Repos/Hackathon_testing/Data/Filt_results_TEST_DA_TEST2.RDS")
unfilt_results <- readRDS("/home/jacob/GitHub_Repos/Hackathon_testing/Data/Unfilt_results_TEST_DA_TEST2.RDS")

unfilt_study_tab  <- readRDS("/home/jacob/GitHub_Repos/Hackathon_testing/Data/unfilt_study_tab_TEST_DA_TEST2.RDS")
filt_study_tab <- readRDS("/home/jacob/GitHub_Repos/Hackathon_testing/Data/filt_study_tab_TEST_DA_TEST2.RDS")

```


## Calculate dataset stats

### Function
```{r, function}
Calc_Other_community_metrics <- function(List_tab){
  Dataset_Char <- list()
  Dataset_Char[["rare"]] <- list()
  Dataset_Char[["nonrare"]] <- list()
  ## Calculate Sparsity
  
  message("Calculating Sparsity")
  ## deal with rare tables first
  for(study in names(List_tab[["rare"]])){
    #get current study table
    study_table <- List_tab[["rare"]][[study]]
    #get total number of cells
    total_cells <- dim(study_table)[1] * dim(study_table)[2]
    #get sparsity
    Sparsity <- length(which(study_table==0))/total_cells
    #assign Sparsity to data.
    Dataset_Char[["rare"]][[study]][["Sparsity"]] <- Sparsity
  }
  ## deal with nonrare tables
    for(study in names(List_tab[["nonrare"]])){
    #get current study table
    study_table <- List_tab[["nonrare"]][[study]]
    #get total number of cells
    total_cells <- dim(study_table)[1] * dim(study_table)[2]
    #get sparsity
    Sparsity <- length(which(study_table==0))/total_cells
    #assign Sparsity to data.
    Dataset_Char[["nonrare"]][[study]][["Sparsity"]] <- Sparsity
  }
  
  message("Calculating Number of Features")
  ## Calculate number of features
  # deal with rare tables
  for(study in names(List_tab[["rare"]])){
    study_table <- List_tab[["rare"]][[study]]
    #tables are features x samples
    num_feats <- dim(study_table)[1]
    
    Dataset_Char[["rare"]][[study]][["Number of Features"]] <- num_feats
  }
  # deal with nonrare tables
  for(study in names(List_tab[["nonrare"]])){
    study_table <- List_tab[["nonrare"]][[study]]
    num_feats <- dim(study_table)[1]
    Dataset_Char[["nonrare"]][[study]][["Number of Features"]] <- num_feats
  }

  message("Calculating median read depth")
  ## Calculate median read depth
  #deal with rare tables
  for(study in names(List_tab[["rare"]])){
    study_table <- List_tab[["rare"]][[study]]
    median_depth <- median(colSums(study_table))
    Dataset_Char[["rare"]][[study]][["Median Depth"]] <- median_depth
  }
  #deal with nonrare tables
  for(study in names(List_tab[["nonrare"]])){
    study_table <- List_tab[["nonrare"]][[study]]
    median_depth <- median(colSums(study_table))
    Dataset_Char[["nonrare"]][[study]][["Median Depth"]] <- median_depth
  }
  message("Calculating median richness")
  ## Calculate median richness
  #deal with rare tables
  for(study in names(List_tab[["rare"]])){
    study_table <- List_tab[["rare"]][[study]]
    binary_study_table <- study_table
    binary_study_table[binary_study_table > 0] <- 1
    median_richness <- median(colSums(binary_study_table))
    Dataset_Char[["rare"]][[study]][["Median Richness"]] <- median_richness
  }
  #deal with nonrare tables
  for(study in names(List_tab[["nonrare"]])){
    study_table <- List_tab[["nonrare"]][[study]]
    binary_study_table <- study_table
    binary_study_table[binary_study_table > 0] <- 1
    median_richness <- median(colSums(binary_study_table))
    Dataset_Char[["nonrare"]][[study]][["Median Richness"]] <- median_richness
  }
  
  message("Calculating depth range")
  for(study in names(List_tab[["nonrare"]])){
    study_table <- List_tab[["nonrare"]][[study]]
    max_depth <- max(colSums(study_table))
    min_depth <- min(colSums(study_table))
    depth_range <- max_depth - min_depth
    Dataset_Char[["nonrare"]][[study]][["Depth Range"]] <- depth_range
    
  }
  
  message("Calculate CoV for depth range")
  for(study in names(List_tab[["nonrare"]])){
    study_table <- List_tab[["nonrare"]][[study]]
    mean_depth <- mean(colSums(study_table))
    std_depth <- sd(colSums(study_table))
    Cov <- std_depth/mean_depth
    Dataset_Char[["nonrare"]][[study]][["Depth CoV"]] <- Cov
  }
  
  return(Dataset_Char)
}


```

### Calculate
```{r cal_dataset_char}
filt_community_metrics <- Calc_Other_community_metrics(filt_study_tab)
unfilt_community_metrics <- Calc_Other_community_metrics(unfilt_study_tab)

### generate tables
filt_rare_metrics_df <- do.call(rbind, filt_community_metrics[[1]])
filt_nonrare_metrics_df <- do.call(rbind, filt_community_metrics[[2]])

unfilt_rare_metrics_df <- do.call(rbind, unfilt_community_metrics[[1]])
unfilt_nonrare_metrics_df <- do.call(rbind, unfilt_community_metrics[[2]])

#scale 
scale_filt_rare_metrics_df <- scale(filt_rare_metrics_df)
scale_filt_nonrare_metrics_df <- scale(filt_nonrare_metrics_df)
scale_unfilt_rare_metrics_df <- scale(unfilt_rare_metrics_df)
scale_unfilt_nonrare_metrics_df <- scale(unfilt_nonrare_metrics_df)
```

### Get Number of Significant ASVs
```{r num_sig_ASV}
sig_counts <- data.frame(matrix(NA,
                                nrow=length(names(filt_results)),
                                ncol=ncol(filt_results[[1]]$adjP_table) + 1))
rownames(sig_counts) <- names(filt_results)
colnames(sig_counts) <- c("dataset", colnames(filt_results[[1]]$adjP_table))
sig_counts$dataset <- rownames(sig_counts)

filt_sig_counts <- sig_counts
filt_sig_percent <- sig_counts

unfilt_sig_counts <- sig_counts
unfilt_sig_percent <- sig_counts

for(study in rownames(filt_sig_counts)) {
  for(tool_name in colnames(filt_sig_counts)) {
    
    if(tool_name == "dataset") { next }

    if(! tool_name %in% colnames(filt_results[[study]]$adjP_table)) {
      filt_sig_counts[study, tool_name] <- NA
      filt_sig_percent[study, tool_name] <- NA
      next
    }
    
    filt_sig_counts[study, tool_name] <- length(which(filt_results[[study]]$adjP_table[, tool_name] < 0.05))
    
    # For rarified pipelines get total # ASVs from wilcoxonrare and for non-rarified get it from wilcoxonclr table.
    if(tool_name %in% c("lefse", "maaslin2rare", "ttestrare", "wilcoxonrare")) {
      filt_sig_percent[study, tool_name] <- (length(which(filt_results[[study]]$adjP_table[, tool_name] < 0.05)) / dim(filt_study_tab[["rare"]][[study]])[1]) * 100
    } else {
      filt_sig_percent[study, tool_name] <- (length(which(filt_results[[study]]$adjP_table[, tool_name] < 0.05)) / dim(filt_study_tab[["nonrare"]][[study]])[1]) * 100
    }
  }
}


for(study in rownames(unfilt_sig_counts)) {
  for(tool_name in colnames(unfilt_sig_counts)) {
    
    if(tool_name == "dataset") { next }

    if(! tool_name %in% colnames(unfilt_results[[study]]$adjP_table)) {
      unfilt_sig_counts[study, tool_name] <- NA
      unfilt_sig_percent[study, tool_name] <- NA
      next
    }
    
    unfilt_sig_counts[study, tool_name] <- length(which(unfilt_results[[study]]$adjP_table[, tool_name] < 0.05))
    
    # For rarified pipelines get total # ASVs from wilcoxonrare and for non-rarified get it from wilcoxonclr table.
    if(tool_name %in% c("lefse", "maaslin2rare", "ttestrare", "wilcoxonrare")) {
      unfilt_sig_percent[study, tool_name] <- (length(which(unfilt_results[[study]]$adjP_table[, tool_name] < 0.05)) / dim(unfilt_study_tab[["rare"]][[study]])[1]) * 100
    } else {
      unfilt_sig_percent[study, tool_name] <- (length(which(unfilt_results[[study]]$adjP_table[, tool_name] < 0.05)) / dim(unfilt_study_tab[["nonrare"]][[study]])[1]) * 100
    }
  }
}

```

### Mean percent across all datasets

Across all filtered datasets:

```{r print_mean_per_filt}
filt_mean_percent <- print(sort(colSums(filt_sig_percent[, -1], na.rm = TRUE) / (colSums(! is.na(filt_sig_percent[, -1])))))
```

Across all unfiltered datasets:

```{r print_mean_per_unfilt}
unfilt_mean_percent <- print(sort(colSums(unfilt_sig_percent[, -1], na.rm = TRUE) / (colSums(! is.na(unfilt_sig_percent[, -1])))))
```


### HeatMaps

#### Filter
```{r}


filt_sig_percent_scaled <- data.frame(scale(t(filt_sig_percent[, -1]), center = TRUE, scale = TRUE))

hackathon_metadata <- read.table("/home/jacob/GitHub_Repos/Hackathon_testing/Analysis_Scripts/Metadata/2020_06_18_Datasets_Hackathon.txt", header=TRUE, sep="\t", stringsAsFactors = FALSE, quote="")
rownames(hackathon_metadata) <- hackathon_metadata$Dataset.Name

## getting sample sizes from metadata sheet.... I'm not sure if these match exactly with the tables... we should double check this...



hackathon_metadata$log_N <- log(hackathon_metadata$Sample.Size)


aitchison_adonis <- readRDS(file = "/home/gavin/projects/hackathon/aitchison_adonis_no_ob_zupancic.rds")

hackathon_metadata$R.squared <- NA




for(dataset in names(aitchison_adonis)) {
  hackathon_metadata[dataset, "R.squared"] <- aitchison_adonis[[dataset]]$R2[1]
}

hackathon_metadata$log_R.squared <- log(hackathon_metadata$R.squared)

#remove ob_zupanic
fixed_hackathon_metadata <- hackathon_metadata[-23,]

## attach other dataset statistics

sort_filt_nonrare_metrics_df <- filt_nonrare_metrics_df[rownames(fixed_hackathon_metadata),,drop=F]
fixed_hackathon_metadata$Sparsity <- sort_filt_nonrare_metrics_df[,1]
fixed_hackathon_metadata$Richness <- sort_filt_nonrare_metrics_df[,4]
fixed_hackathon_metadata$log_Depth <- log(sort_filt_nonrare_metrics_df[,3])
fixed_hackathon_metadata$log_Depth_range <- log(sort_filt_nonrare_metrics_df[,5])
fixed_hackathon_metadata$CoV_Depth <- sort_filt_nonrare_metrics_df[,6]

colnames(filt_sig_percent_scaled)
test <- Data_set_names[colnames(filt_sig_percent_scaled)]
colnames(filt_sig_percent_scaled) <- test

colnames(filt_sig_percent_scaled)
rownames(filt_sig_percent_scaled)
## fix tool names now...
tool_rename <- tool_names[rownames(filt_sig_percent_scaled)]
rownames(filt_sig_percent_scaled) <- tool_rename

### now fix metadata

rownames(fixed_hackathon_metadata) 
test2 <- Data_set_names[rownames(fixed_hackathon_metadata)]


rownames(fixed_hackathon_metadata) <- test2





Alpha_order_filt <- filt_sig_percent_scaled[,order(colnames(filt_sig_percent_scaled))]

rownames(Alpha_order_filt)


### now fix raw count dataframe

raw_count_df <- t(filt_sig_counts[,-1])
rownames(raw_count_df)
test <- tool_names[rownames(raw_count_df)]
rownames(raw_count_df) <- test

colnames(raw_count_df)
test2 <- Data_set_names[colnames(raw_count_df)]

colnames(raw_count_df) <- test2

order_raw_count_df <- raw_count_df[,colnames(Alpha_order_filt)]

identical(colnames(order_raw_count_df), colnames(Alpha_order_filt))

test <- Metadata_renames[colnames(fixed_hackathon_metadata[30:37])]

colnames(fixed_hackathon_metadata)[30:37] <- Metadata_renames

colnames(fixed_hackathon_metadata)[which(colnames(fixed_hackathon_metadata) == "Log Aitchson's Dist. Effect Size")] <- "log(Aitch. dist. effect size)"
colnames(fixed_hackathon_metadata)[which(colnames(fixed_hackathon_metadata) == "Log Read Depth Range")] <- "log(Read depth range)"
colnames(fixed_hackathon_metadata)[which(colnames(fixed_hackathon_metadata) == "Read Depth Variation")] <- "Read depth variation"
colnames(fixed_hackathon_metadata)[which(colnames(fixed_hackathon_metadata) == "Log Sample Size")] <- "log(Sample size)"

filt_ASV_nums <- pheatmap(t(Alpha_order_filt),
         clustering_method = "complete",
         legend=TRUE,
         display_numbers=t(order_raw_count_df),
         annotation_row=fixed_hackathon_metadata[, c("log(Sample size)", "log(Aitch. dist. effect size)", 
                                                     "Sparsity", "Richness", "Read depth variation", 
                                                     "log(Read depth range)"), drop=FALSE],
         annotation_legend=FALSE,
         legend_labels = "Percent of Significant Features",
         treeheight_col = 0,
         cluster_cols = F,
         cluster_rows = F,
         main="Filtered",
         angle_col=315)
```

#### UnFilter
```{r}

unfilt_sig_percent_scaled <- data.frame(scale(t(unfilt_sig_percent[, -1]), center = TRUE, scale = TRUE))

hackathon_metadata <- read.table("/home/jacob/GitHub_Repos/Hackathon_testing/Analysis_Scripts/Metadata/2020_06_18_Datasets_Hackathon.txt", header=TRUE, sep="\t", stringsAsFactors = FALSE, quote="")
rownames(hackathon_metadata) <- hackathon_metadata$Dataset.Name

hackathon_metadata$log_N <- log(hackathon_metadata$Sample.Size)

aitchison_adonis <- readRDS(file = "/home/gavin/projects/hackathon/aitchison_adonis_no_ob_zupancic.rds")

hackathon_metadata$R.squared <- NA

for(dataset in names(aitchison_adonis)) {
  hackathon_metadata[dataset, "R.squared"] <- aitchison_adonis[[dataset]]$R2[1]
}

hackathon_metadata$log_R.squared <- log(hackathon_metadata$R.squared)

#remove ob_zupanic
fixed_hackathon_metadata <- hackathon_metadata[-23,]

## attach other dataset statistics

sort_unfilt_nonrare_metrics_df <- unfilt_nonrare_metrics_df[rownames(fixed_hackathon_metadata),,drop=F]
fixed_hackathon_metadata$Sparsity <- sort_unfilt_nonrare_metrics_df[,1]
fixed_hackathon_metadata$Richness <- sort_unfilt_nonrare_metrics_df[,4]
fixed_hackathon_metadata$log_Depth <- log(sort_unfilt_nonrare_metrics_df[,3])
fixed_hackathon_metadata$Log_Depth_range <- log(sort_unfilt_nonrare_metrics_df[,5])
fixed_hackathon_metadata$CoV_Depth <- sort_unfilt_nonrare_metrics_df[,6]


test <- Data_set_names[colnames(unfilt_sig_percent_scaled)]
colnames(unfilt_sig_percent_scaled) <- test

rownames(unfilt_sig_percent_scaled)
test2 <- tool_names[rownames(unfilt_sig_percent_scaled)]
rownames(unfilt_sig_percent_scaled) <- test2

Alpha_order_unfilt <- unfilt_sig_percent_scaled[,order(colnames(unfilt_sig_percent_scaled))]


raw_counts_unfilt <- t(unfilt_sig_counts[,-1])
rownames(raw_counts_unfilt)
test2 <- tool_names[rownames(raw_counts_unfilt)]
rownames(raw_counts_unfilt) <- test2

colnames(raw_counts_unfilt)
test <- Data_set_names[colnames(raw_counts_unfilt)]

colnames(raw_counts_unfilt) <- test

fix_raw_counts_unfilt <- raw_counts_unfilt[,colnames(Alpha_order_unfilt)]


identical(rownames(fix_raw_counts_unfilt), rownames(Alpha_order_unfilt))
# fix metadata

test2 <- Data_set_names[rownames(fixed_hackathon_metadata)]


rownames(fixed_hackathon_metadata) <- test2


test <- Metadata_renames[colnames(fixed_hackathon_metadata[30:37])]

colnames(fixed_hackathon_metadata)[30:37] <- Metadata_renames

colnames(fixed_hackathon_metadata)[which(colnames(fixed_hackathon_metadata) == "Log Aitchson's Dist. Effect Size")] <- "log(Aitch. dist. effect size)"
colnames(fixed_hackathon_metadata)[which(colnames(fixed_hackathon_metadata) == "Log Read Depth Range")] <- "log(Read depth range)"
colnames(fixed_hackathon_metadata)[which(colnames(fixed_hackathon_metadata) == "Read Depth Variation")] <- "Read depth variation"
colnames(fixed_hackathon_metadata)[which(colnames(fixed_hackathon_metadata) == "Log Sample Size")] <- "log(Sample size)"


unfilt_ASV_nums <- pheatmap(t(Alpha_order_unfilt),
         clustering_method = "complete",
         legend=TRUE,
         display_numbers=t(fix_raw_counts_unfilt),
         annotation_row=fixed_hackathon_metadata[, c("log(Sample size)", "log(Aitch. dist. effect size)", 
                                                     "Sparsity", "Richness", "Read depth variation", 
                                                     "log(Read depth range)"), drop=FALSE],
         annotation_legend=FALSE,
         treeheight_col = 0,
         cluster_cols = F,
         cluster_rows = F,
         main="Unfiltered",
         angle_col=315)
```




## Final Plot
```{r}
figure1_plot <- cowplot::plot_grid(unfilt_ASV_nums[[4]], filt_ASV_nums[[4]], nrow=2, labels=c('A', 'B'))

ggsave(filename=paste(display_items_out, "Main_figures", "Figure1.pdf", sep="/"),
       plot = figure1_plot, width = 9, height=13, units="in", dpi=600)

ggsave(filename=paste(display_items_out, "Main_figures", "Figure1.png", sep="/"),
       plot = figure1_plot, width = 9, height=13, units="in", dpi=150)
```



## Supplemental Correlation Figure
```{r}

library(corrplot)

# Filter
filt_sig_percent_fix <- filt_sig_percent[,-1]

#fix dataset names
rownames(filt_sig_percent_fix) <- Data_set_names[rownames(filt_sig_percent_fix)]

fixed_hackathon_metadata_filt <- fixed_hackathon_metadata[rownames(filt_sig_percent_fix),]


filt_rhos <- list()
filt_ps <- list()

for(i in 1:length(colnames(filt_sig_percent_fix))){
  ##go through each Tool
  temp_pvals <- vector()
  temp_rhos <- vector()
  
  for(j in 1:8){
    message(j)
    # run correlation of column
    temp_pvals[[j]] <- cor.test(filt_sig_percent_fix[,i], fixed_hackathon_metadata_filt[,(j+29)], method="spearman", exact=F)$p.value
    temp_rhos[[j]] <- cor.test(filt_sig_percent_fix[,i], fixed_hackathon_metadata_filt[,(j+29)], method="spearman", exact=F)$estimate
  }
  
  filt_rhos[[i]] <- temp_rhos
  filt_ps[[i]] <- temp_pvals
}

filt_rhos_df <- do.call(rbind, filt_rhos)
filt_pvals_df <- do.call(rbind, filt_ps)

rownames(filt_rhos_df) <- colnames(filt_sig_percent_fix)

rownames(filt_rhos_df) <- tool_rename[rownames(filt_rhos_df)]

rownames(filt_pvals_df) <- colnames(filt_sig_percent_fix)

colnames(filt_rhos_df) <- colnames(fixed_hackathon_metadata_filt)[30:37]
colnames(filt_pvals_df) <- colnames(fixed_hackathon_metadata_filt)[30:37]

corrplot(filt_rhos_df[,-3], p.mat = filt_pvals_df[,-3], sig.level = 0.05, insig = "blank", full_col = FALSE)
# Saved manually as 8.5x11 inches as Supp_figures/Supp_Corrplot_Filt.pdf


### unfilt
unfilt_sig_percent_fix <- unfilt_sig_percent[,-1]

#fix dataset names
rownames(unfilt_sig_percent_fix) <- Data_set_names[rownames(unfilt_sig_percent_fix)]

fixed_hackathon_metadata_unfilt <- fixed_hackathon_metadata[rownames(unfilt_sig_percent_fix),]


unfilt_rhos <- list()
unfilt_ps <- list()

for(i in 1:length(colnames(unfilt_sig_percent_fix))){
  ##go through each Tool
  temp_pvals <- vector()
  temp_rhos <- vector()
  
  for(j in 1:8){
    message(j+29)
    # run correlation of column
    temp_pvals[[j]] <- cor.test(unfilt_sig_percent_fix[,i], fixed_hackathon_metadata_unfilt[,(j+29)], method="spearman", exact=F)$p.value
    temp_rhos[[j]] <- cor.test(unfilt_sig_percent_fix[,i], fixed_hackathon_metadata_unfilt[,(j+29)], method="spearman", exact=F)$estimate
  }
  
  unfilt_rhos[[i]] <- temp_rhos
  unfilt_ps[[i]] <- temp_pvals
}

unfilt_rhos_df <- do.call(rbind, unfilt_rhos)
unfilt_pvals_df <- do.call(rbind, unfilt_ps)

rownames(unfilt_rhos_df) <- colnames(unfilt_sig_percent_fix)

rownames(unfilt_rhos_df) <- tool_rename[rownames(unfilt_rhos_df)]

rownames(unfilt_pvals_df) <- colnames(unfilt_sig_percent_fix)

colnames(unfilt_rhos_df) <- colnames(fixed_hackathon_metadata_unfilt)[30:37]
colnames(unfilt_pvals_df) <- colnames(fixed_hackathon_metadata_unfilt)[30:37]

corrplot(as.matrix(unfilt_rhos_df[,-3]), full_col = FALSE, p.mat = unfilt_pvals_df[,-3], sig.level = 0.05, insig = "blank")
# Saved manually as 8.5x11 inches as Supp_figures/Supp_Corrplot_Unfilt.pdf
```

## Rho values to report in main text
```{r}
print("Filtered")
sort(filt_rhos_df[, "Aitchson's Dist. Effect Size"])

print("========================")

print("Unfiltered")
sort(unfilt_rhos_df[, "Aitchson's Dist. Effect Size"])

```